{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5785cdf",
   "metadata": {
    "id": "a5785cdf"
   },
   "source": [
    "<center><h1>Scraping LeetCode Problem Details Using Python, Selenium & BeautifulSoup</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdb8ff",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <a href=\"https://leetcode.com/\">\n",
    "         <img src=\"https://i.imgur.com/9aLyuqU.png\" width=100px style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "    </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2508484",
   "metadata": {
    "id": "f2508484"
   },
   "source": [
    "<h2> Web Scraping </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c80e5",
   "metadata": {},
   "source": [
    "- Web Scraping is the process of deriving data from a website using a computer program. The program scrapes for HTML code, which is the standard markup that websites display content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570c48b",
   "metadata": {},
   "source": [
    "<h2> List of the tools used in this Notebook </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8a93c",
   "metadata": {},
   "source": [
    "- **Python**\n",
    "- **Selenium** \n",
    "- **Beautiful Soup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6109ab",
   "metadata": {},
   "source": [
    "<h2> About LeetCode </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c726e7",
   "metadata": {
    "id": "16c726e7"
   },
   "source": [
    "- LeetCode is your ultimate platform for skill enhancement, knowledge expansion, and effective technical interview preparation, offering a vast library of 3000+ coding problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a322710",
   "metadata": {},
   "source": [
    " <center><a href=\"https://leetcode.com/problemset/all/\"><img src=\"https://i.imgur.com/lyJxDDz.png\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:10px;\" width=800px></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794bf79",
   "metadata": {},
   "source": [
    "<center><h3> First Part </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dca5d3",
   "metadata": {},
   "source": [
    "- First we will be scraping all problems list.\n",
    "- The first section of the website is the problem set table that contains all the problems.\n",
    "- There are over 3000+ problems, distributed over 60 pages.The first section of the website is the problem set table that contains all the problems.\n",
    "\n",
    "<center><a href=\"https://leetcode.com/problemset/all/\"><img src=\"https://i.imgur.com/giQr7yk.png\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:10px;\" width=75%></a></center>\n",
    " \n",
    "**The details we are scraping from the first section are:**\n",
    "\n",
    "1. Title\n",
    "2. Problem_URL\n",
    "3. Solution_URL\n",
    "4. Acceptance\n",
    "5. Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181870c",
   "metadata": {},
   "source": [
    "<center><h3> Second Part </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f3243",
   "metadata": {},
   "source": [
    "- Second we will scrape the problem information from every problem page.\n",
    "- we need to scrape 3000 problem pages because we have 3000 problems.\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 1; margin-right: 10px;\">\n",
    "    <a href=\"https://leetcode.com/problems/4sum/\">\n",
    "      <img src=\"https://i.imgur.com/5BwD2nb.png\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:10px;width: 100%;\">\n",
    "    </a>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; margin-right: 10px;\">\n",
    "    <a href=\"https://leetcode.com/problems/4sum/\">\n",
    "      <img src=\"https://i.imgur.com/ozWPDVj.png\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:10px;width: 100%;\">\n",
    "    </a>\n",
    "  </div>\n",
    "  <div style=\"flex: 1;\">\n",
    "    <a href=\"https://leetcode.com/problems/4sum/\">\n",
    "      <img src=\"https://i.imgur.com/QETFefW.png\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:10px;width: 100%;\">\n",
    "    </a>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "**The details we are scraping from the second section are:**\n",
    "\n",
    "1. Premium Status\n",
    "2. Title\n",
    "3. Problem Description \n",
    "4. Topic Tags\n",
    "5. Accepted\n",
    "6. Submission\n",
    "7. Solution\n",
    "8. Discussion Count\n",
    "9. Likes\n",
    "10. Dislikes\n",
    "11. Similar Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40770fcf",
   "metadata": {},
   "source": [
    "<center><h3> Third Part </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c7505",
   "metadata": {},
   "source": [
    "- This is the last part.\n",
    "- Combine both part of The DataFrame.\n",
    "- Analyzing Data Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9485b0e",
   "metadata": {},
   "source": [
    "<h2> Scraping First Section: </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821948d1",
   "metadata": {
    "id": "821948d1"
   },
   "source": [
    "`Importing And Installing Relevant Libraries`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247033b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 5305,
     "status": "error",
     "timestamp": 1688128208706,
     "user": {
      "displayName": "Joyesh Meshram",
      "userId": "08771931488029831946"
     },
     "user_tz": -330
    },
    "id": "6247033b",
    "outputId": "ec252733-4541-4566-f349-87e52d262a56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time  # For handling time-related functions\n",
    "import re  # For regular expressions\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from bs4 import BeautifulSoup  # For web scraping\n",
    "from selenium import webdriver  # For browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # For configuring the ChromeDriver service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7a396",
   "metadata": {},
   "source": [
    "`Functions that help scraping first section`:\n",
    "\n",
    "1. `get_driver`:\n",
    "   - Returns a configured instance of the Chrome WebDriver for web scraping.\n",
    "---\n",
    "2. `get_page_source(url, delay=10)`:\n",
    "   - Gets the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "   - Optional delay parameter to wait for the page to load.\n",
    "---\n",
    "3. `get_titles(page_source, first_page=False)`:\n",
    "   - Extracts titles of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index.\n",
    "---\n",
    "4. `get_problems_URL(page_source, first_page=False)`:\n",
    "   - Extracts URLs of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index.\n",
    "---\n",
    "5. `get_acceptances_difficulties(page_source, first_page=False)`:\n",
    "   - Extracts acceptance rates and difficulties of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index.\n",
    "---\n",
    "6. `get_single_page_df(url, first_page=False)`:\n",
    "   - Combines the above functions to create a Pandas DataFrame for a single page of problems.\n",
    "---\n",
    "7. `get_multiple_page_df(start=1, end=60)`:\n",
    "   - Calls `get_single_page_df` for a range of pages and concatenates the results into a single DataFrame.\n",
    "---\n",
    "8. `scrape(start=1, end=60, file_name='part1.csv')`:\n",
    "   - Initiates the scraping process, saving the resulting DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce46387",
   "metadata": {},
   "source": [
    "1. `get_driver`:\n",
    "   - Returns a configured instance of the Chrome WebDriver for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de384384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    \"\"\"\n",
    "    Creates and configures a Chrome WebDriver instance for web scraping.\n",
    "\n",
    "    Returns:\n",
    "        WebDriver: Configured instance of the Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    # Set the path to the ChromeDriver executable\n",
    "    service = Service('D:/chromedriver-win64/chromedriver.exe')\n",
    "\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Ignore certificate errors\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    \n",
    "    # Start the browser in maximized mode\n",
    "    options.add_argument('--start-maximized')\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified service and options\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Return the WebDriver instance\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe86e1",
   "metadata": {},
   "source": [
    "2. `get_page_source(url, delay=10)`:\n",
    "   - Gets the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "   - Optional delay parameter to wait for the page to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page_source(url, delay=10):\n",
    "    \"\"\"\n",
    "    Retrieves the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape.\n",
    "        delay (int, optional): Delay in seconds to wait for the page to load. Default is 10 seconds.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: The BeautifulSoup object representing the page source.\n",
    "    \"\"\"\n",
    "    # Get a Chrome WebDriver instance\n",
    "    driver = get_driver()\n",
    "\n",
    "    # Open the specified URL in the browser\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow time for the page to load (adjust delay as needed)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the page source using BeautifulSoup for parsing\n",
    "    page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver to release resources\n",
    "    driver.quit()\n",
    "\n",
    "    # Return the parsed page source\n",
    "    return page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb24fb1",
   "metadata": {},
   "source": [
    "3. `get_titles(page_source, first_page=False)`:\n",
    "   - Extracts titles of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b7b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_titles(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts titles of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted titles.\n",
    "    \"\"\"\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Find all title elements using BeautifulSoup\n",
    "    title_elements = page_source.find_all(\n",
    "        'a',\n",
    "        class_=[\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s',\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s opacity-60'\n",
    "        ]\n",
    "    )[start_index:]\n",
    "\n",
    "    # Extract text from title elements and store in a list\n",
    "    titles = [title_element.text for title_element in title_elements]\n",
    "\n",
    "    # Return the list of titles\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd89b7",
   "metadata": {},
   "source": [
    "4. `get_problems_URL(page_source, first_page=False)`:\n",
    "   - Extracts URLs of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4775d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_problems_URL(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts URLs of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted problem URLs.\n",
    "    \"\"\"\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Find all problem elements with an 'a' tag and an 'href' attribute using BeautifulSoup\n",
    "    problem_elements = page_source.find_all('a', href=True, class_=[\n",
    "        'h-5 hover:text-blue-s dark:hover:text-dark-blue-s',\n",
    "        'h-5 hover:text-blue-s dark:hover:text-dark-blue-s opacity-60'\n",
    "    ])[start_index:]\n",
    "\n",
    "    # Extract the 'href' attribute values from the problem elements and store in a list\n",
    "    problems_url = [el['href'] for el in problem_elements]\n",
    "\n",
    "    # Return the list of problem URLs\n",
    "    return problems_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ede3b",
   "metadata": {},
   "source": [
    "5. `get_acceptances_difficulties(page_source, first_page=False)`:\n",
    "   - Extracts acceptance rates and difficulties of problems from the page source.\n",
    "   - Optional parameter `first_page` to handle starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08095d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_acceptances_difficulties(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts acceptances and difficulties of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists - acceptances and difficulties.\n",
    "    \"\"\"\n",
    "    # Find all div elements with the specified class using BeautifulSoup\n",
    "    div_elements = page_source.find_all('div', class_='mx-2 flex items-center py-[11px]')\n",
    "\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Extract text from the span elements within the div elements and store in a list\n",
    "    items = [\n",
    "        span_element.text.strip()\n",
    "        for div_element in div_elements\n",
    "        for span_element in [div_element.find('span')]\n",
    "        if span_element\n",
    "    ]\n",
    "\n",
    "    # Separate the items into acceptances and difficulties lists\n",
    "    acceptances, difficulties = [], []\n",
    "    for item in items:\n",
    "        if item:\n",
    "            (acceptances if item.endswith('%') else difficulties).append(item)\n",
    "\n",
    "    # Adjust lists based on the starting index\n",
    "    acceptances = acceptances[start_index:]\n",
    "    difficulties = difficulties[start_index:]\n",
    "\n",
    "    # Return the lists of acceptances and difficulties\n",
    "    return acceptances, difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036afc04",
   "metadata": {},
   "source": [
    "6. `get_single_page_df(url, first_page=False)`:\n",
    "   - Combines the above functions to create a Pandas DataFrame for a single page of problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf93e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_single_page_df(url, first_page=False):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame for a single page of problems.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scrape.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing titles, problem URLs, acceptances, and difficulties.\n",
    "    \"\"\"\n",
    "    # Get the page source for the specified URL\n",
    "    page_source = get_page_source(url)\n",
    "\n",
    "    # Extract titles, problem URLs, acceptances, and difficulties from the page source\n",
    "    titles = get_titles(page_source, first_page)\n",
    "    problems_url = get_problems_URL(page_source, first_page)\n",
    "    acceptances, difficulties = get_acceptances_difficulties(page_source, first_page)\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    data = {\n",
    "        'title': titles,\n",
    "        'problem_URL': problems_url,\n",
    "        'acceptance': acceptances,\n",
    "        'difficulty': difficulties\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame using the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2217c",
   "metadata": {},
   "source": [
    "7. `get_multiple_page_df(start=1, end=60)`:\n",
    "   - Calls `get_single_page_df` for a range of pages and concatenates the results into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22b3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_multiple_page_df(start=1, end=60):\n",
    "    \"\"\"\n",
    "    Calls get_single_page_df for a range of pages and concatenates the results into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        start (int, optional): The starting page. Default is 1.\n",
    "        end (int, optional): The ending page. Default is 60.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing titles, problem URLs, acceptances, and difficulties for multiple pages.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store DataFrames for each page\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Set the flag for the first page\n",
    "    first_page = True if start == 1 else False\n",
    "\n",
    "    # Iterate over the specified range of pages\n",
    "    for i in range(start, end + 1):\n",
    "        # Construct the URL for the current page\n",
    "        url = 'https://leetcode.com/problemset/all/?page=' + str(i)\n",
    "\n",
    "        # Get the DataFrame for the current page and append it to the list\n",
    "        df = get_single_page_df(url, first_page)\n",
    "        list_of_dfs.append(df)\n",
    "\n",
    "        # Update the first_page flag for subsequent pages\n",
    "        first_page = False\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28148d",
   "metadata": {},
   "source": [
    "8. `scrape(start=1, end=60, file_name='part1.csv')`:\n",
    "   - Initiates the scraping process, saving the resulting DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c4610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(start=1, end=60, file_name='part1.csv'):\n",
    "    \"\"\"\n",
    "    Initiates the scraping process, saving the resulting DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        start (int, optional): The starting page for scraping. Default is 1.\n",
    "        end (int, optional): The ending page for scraping. Default is 60.\n",
    "        file_name (str, optional): The name of the CSV file to save the scraped data. Default is 'part1.csv'.\n",
    "    \"\"\"\n",
    "    # Get the DataFrame by scraping multiple pages\n",
    "    df = get_multiple_page_df(start, end)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d21c0f",
   "metadata": {},
   "source": [
    "`Running Web-Scraping Process`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a4797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape(start=1, end=60, file_name='x1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e204b9",
   "metadata": {},
   "source": [
    "<p>This is the sample Dataframe : </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c7781",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/bj5NPGA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04f95f",
   "metadata": {},
   "source": [
    "`Data preprocessing in first section`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248778b",
   "metadata": {},
   "source": [
    "- We need to convert `problem_URL` to proper url format.\n",
    "- We need to add `solution_URL` column in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('x1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93e2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['problem_URL'] = df1['problem_URL'].apply(lambda x: f'{\"https://leetcode.com\"}{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74868a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['solution_URL'] = df1['problem_URL'].apply(lambda x: f'{x}{\"/solution\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d0ce8",
   "metadata": {},
   "source": [
    "<p>Now we are done with preprocssing: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f475f0",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/7gUlYSg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54818c",
   "metadata": {
    "id": "f881092d"
   },
   "source": [
    "<center> <h2 style=\"color:blue\"> Result of First Section: </h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352cb9b",
   "metadata": {},
   "source": [
    "<p>This is the sample Dataframe : </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26332d",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Z70wkgZ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e49795",
   "metadata": {},
   "source": [
    "<h2> Scraping Second Section: </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c53fed",
   "metadata": {},
   "source": [
    "`Functions that help scraping second section`:\n",
    "\n",
    "1. `get_title(page_source)`:\n",
    "   - Extracts the title from the given page source.\n",
    "---\n",
    "2. `get_problem_description(page_source)`:\n",
    "   - Extracts the problem description from the given page source.\n",
    "---\n",
    "3. `get_topic_tags(page_source)`:\n",
    "    - Extracts topic tags from the given page source.\n",
    "---\n",
    "4. `get_accepted(page_source)`:\n",
    "    - Extracts the number of accepted submissions from the given page source.\n",
    "---\n",
    "5. `get_submission(page_source)`:\n",
    "    - Extracts the number of total submissions from the given page source.\n",
    "---\n",
    "6. `get_solution(page_source)`:\n",
    "    - Extracts the solution count from the given page source.\n",
    "---\n",
    "7. `get_discussion_count(page_source)`:\n",
    "    - Extracts the count of discussions from the given page source.\n",
    "---\n",
    "8. `get_likes(page_source)`:\n",
    "    - Extracts the number of likes from the given page source.\n",
    "---\n",
    "9. `get_dislikes(page_source)`:\n",
    "    - Extracts the number of dislikes from the given page source.\n",
    "---\n",
    "10. `get_similar_questions(page_source)`:\n",
    "    - Extracts a list of similar questions from the given page source.\n",
    "---\n",
    "11. `get_page_source(url, delay=10)`:\n",
    "    - Retrieves the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "---\n",
    "12. `get_ispremium(page_source)`:\n",
    "    - Checks if the page indicates a premium status.\n",
    "---\n",
    "13. `scrape(df1, start=1, end=3000, file_name='x2.csv')`:\n",
    "    - Scrapes data for a range of links from the provided DataFrame and saves it to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1ed85",
   "metadata": {},
   "source": [
    "1. `get_title(page_source)`:\n",
    "   - Extracts the title from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e60fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_title(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the title from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted title text.\n",
    "    \"\"\"\n",
    "    # Find the title element using BeautifulSoup\n",
    "    title_element = page_source.find(\n",
    "        'a',\n",
    "        class_='mr-2 text-label-1 dark:text-dark-label-1 hover:text-label-1 dark:hover:text-dark-label-1 text-lg font-medium'\n",
    "    )\n",
    "\n",
    "    # Extract the text content from the title element\n",
    "    title = title_element.text\n",
    "\n",
    "    # Return the title\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebb859",
   "metadata": {},
   "source": [
    "2. `get_problem_description(page_source)`:\n",
    "   - Extracts the problem description from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08e106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_problem_description(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the problem description from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted problem description text.\n",
    "    \"\"\"\n",
    "    # Find the div element containing the problem description using BeautifulSoup\n",
    "    description_element = page_source.find(\n",
    "        'div',\n",
    "        class_='xFUwe'\n",
    "    )\n",
    "\n",
    "    # Extract the text content from the description element\n",
    "    description_text = description_element.text\n",
    "\n",
    "    # Return the problem description\n",
    "    return description_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7e113",
   "metadata": {},
   "source": [
    "3. `get_topic_tags(page_source)`:\n",
    "   - Extracts topic tags from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38d99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_topic_tags(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the topic tags from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of extracted topic tags.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store topic tags\n",
    "    topic_tags = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    topic_tag_elements = page_source.find_all('a',\n",
    "                 class_='mr-4 rounded-xl px-2 py-1 text-xs transition-colors text-label-2 dark:text-dark-label-2 hover:text-label-2 dark:hover:text-dark-label-2 bg-fill-3 dark:bg-dark-fill-3 hover:bg-fill-2 dark:hover:bg-dark-fill-2') \n",
    "\n",
    "    # Extract text content from each topic tag element and append to the list\n",
    "    for topic_tag_element in topic_tag_elements:\n",
    "        topic_tag = topic_tag_element.text\n",
    "        topic_tags.append(topic_tag)\n",
    "\n",
    "    # Join the list of topic tags into a comma-separated string\n",
    "    topic_tags_str = ', '.join(f\"'{item}'\" for item in topic_tags)\n",
    "\n",
    "    # Return the formatted string of topic tags\n",
    "    return topic_tags_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38390315",
   "metadata": {},
   "source": [
    "4. `get_accepted(page_source)`:\n",
    "   - Extracts the number of accepted submissions from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719c504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_accepted(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of accepted submissions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        Tag: The BeautifulSoup Tag object containing the information about accepted submissions.\n",
    "    \"\"\"\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    accepted_elements = page_source.find_all('div', class_='text-label-1 dark:text-dark-label-1 text-sm font-medium')\n",
    "\n",
    "    # Return the first element (it contains the information about accepted submissions)\n",
    "    return accepted_elements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abb891",
   "metadata": {},
   "source": [
    "5. `get_submission(page_source)`:\n",
    "   - Extracts the number of total submissions from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47ffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_submission(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of total submissions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        Tag: The BeautifulSoup Tag object containing the information about total submissions.\n",
    "    \"\"\"\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    submission_elements = page_source.find_all('div', class_='text-label-1 dark:text-dark-label-1 text-sm font-medium')\n",
    "\n",
    "    # Return the second element (it contains the information about total submissions)\n",
    "    return submission_elements[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c4666",
   "metadata": {},
   "source": [
    "6. `get_solution(page_source)`:\n",
    "   - Extracts the solution count from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a90d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_solution(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the solution count from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted solution.\n",
    "    \"\"\"\n",
    "    # Find the element with a link containing 'solutions' using BeautifulSoup\n",
    "    solutions_element = page_source.find(href=re.compile(\"solutions\"))\n",
    "\n",
    "    # Use regular expression to extract the solution type from the text\n",
    "    solution = re.findall(r\"\\((.*?)\\)\", solutions_element.text)[0]\n",
    "\n",
    "    # Return the extracted solution count\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b1508",
   "metadata": {},
   "source": [
    "7. `get_discussion_count(page_source)`:\n",
    "   - Extracts the count of discussions from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fda21e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_discussion_count(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the count of discussions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted count of discussions.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    discussion_count_element = page_source.find_all('div', class_='flex-1 text-sm leading-[22px]')[0]\n",
    "\n",
    "    # Use regular expression to extract the count of discussions from the element's text\n",
    "    discussion_count = re.findall(r\"\\((.*?)\\)\", discussion_count_element.text)[0]\n",
    "\n",
    "    # Return the extracted count of discussions\n",
    "    return discussion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fb130",
   "metadata": {},
   "source": [
    "8. `get_likes(page_source)`:\n",
    "   - Extracts the number of likes from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4232b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_likes(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of likes from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted number of likes.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    likes_element = page_source.find_all('div', class_='text-lg text-gray-6 dark:text-dark-gray-6')[0]\n",
    "\n",
    "    # Find the next sibling element and extract the text content\n",
    "    likes_count = likes_element.find_next_sibling().text\n",
    "\n",
    "    # Return the extracted number of likes\n",
    "    return likes_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582cfccc",
   "metadata": {},
   "source": [
    "9. `get_dislikes(page_source)`:\n",
    "   - Extracts the number of dislikes from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17a9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dislikes(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of dislikes from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted number of dislikes.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    dislikes_element = page_source.find_all('div', class_='text-lg text-gray-6 dark:text-dark-gray-6')[1]\n",
    "\n",
    "    # Find the next sibling element and extract the text content\n",
    "    dislikes_count = dislikes_element.find_next_sibling().text\n",
    "\n",
    "    # Return the extracted number of dislikes\n",
    "    return dislikes_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5666e",
   "metadata": {},
   "source": [
    "10. `get_similar_questions(page_source)`:\n",
    "    - Extracts a list of similar questions from the given page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aaea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_similar_questions(page_source):\n",
    "    \"\"\"\n",
    "    Extracts a list of similar questions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of extracted similar questions.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store similar questions\n",
    "    similar_questions = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    similar_question_elements = page_source.find_all('a', class_='text-sm font-medium transition-none text-label-1 dark:text-dark-label-1 hover:text-blue-s dark:hover:text-dark-blue-s')\n",
    "\n",
    "    # Extract text content from each similar question element and append to the list\n",
    "    for similar_question_element in similar_question_elements:\n",
    "        similar_question = similar_question_element.text\n",
    "        similar_questions.append(similar_question)\n",
    "\n",
    "    # Join the list of similar questions into a comma-separated string\n",
    "    similar_questions_str = ', '.join(f\"'{item}'\" for item in similar_questions)\n",
    "\n",
    "    # Return the formatted string of similar questions\n",
    "    return similar_questions_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07048628",
   "metadata": {},
   "source": [
    "11. `get_page_source(url, delay=10)`:\n",
    "    - Retrieves the page source of a specified URL using Selenium and BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecf835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page_source(driver, url, delay=10):\n",
    "    \"\"\"\n",
    "    Retrieves the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): The Selenium WebDriver object.\n",
    "        url (str): The URL to scrape.\n",
    "        delay (int, optional): Delay in seconds to wait for the page to load. Default is 10 seconds.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: The BeautifulSoup object representing the page source.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the specified URL in the browser\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow time for the page to load (adjust delay as needed)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the page source using BeautifulSoup for parsing\n",
    "    page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Return the parsed page source\n",
    "    return page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca7cb8",
   "metadata": {},
   "source": [
    "12. `get_ispremium(page_source)`:\n",
    "    - Checks if the page indicates a premium status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ebcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_is_premium(page_source):\n",
    "    \"\"\"\n",
    "    Checks if the page indicates a premium status.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: 'True' if premium, 'False' otherwise.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    premium_element = page_source.find('div', class_='text-md mb-6 text-center text-label-2 dark:text-dark-label-2')\n",
    "\n",
    "    # Determine premium status based on the existence of the element\n",
    "    is_premium = 'True' if premium_element else 'False'\n",
    "\n",
    "    # Return the premium status\n",
    "    return is_premium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772e4db",
   "metadata": {},
   "source": [
    "13. `scrape(df1, start=1, end=3000, file_name='x2.csv')`:\n",
    "    - Scrapes data for a range of links from the provided DataFrame and saves it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9fcb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(df1, start=1, end=3000, file_name='x2.csv'):\n",
    "    \"\"\"\n",
    "    Scrapes data for a range of links from the provided DataFrame and saves it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): The DataFrame containing problem URLs.\n",
    "        start (int, optional): The starting index for scraping. Default is 1.\n",
    "        end (int, optional): The ending index for scraping. Default is 3000.\n",
    "        file_name (str, optional): The name of the CSV file to save the scraped data. Default is 'x2.csv'.\n",
    "    \"\"\"\n",
    "    # Extract links for the specified range from the DataFrame\n",
    "    links = df1['problem_URL'][start - 1:end]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for each link\n",
    "    dfs = []\n",
    "    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    service = Service('D:/chromedriver-win64/chromedriver.exe')\n",
    "\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')  # Ignore certificate errors\n",
    "    options.add_argument('--start-maximized')  # Start the browser in maximized mode\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified service and options\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Iterate over the links and scrape data\n",
    "    for link in links:\n",
    "        i = 0\n",
    "        # Get the page source for the current link\n",
    "        page_source = get_page_source(driver, link, delay=10)\n",
    "\n",
    "        # Create a dictionary to store scraped data\n",
    "        data = {'is_premium': get_is_premium(page_source)}\n",
    "\n",
    "        # Check if the problem is not premium before scraping additional data\n",
    "        if data['is_premium'] == 'False':\n",
    "            # Update the data dictionary with additional scraped data\n",
    "            data.update({\n",
    "                'title': get_title(page_source),\n",
    "                'problem_description': get_problem_description(page_source),\n",
    "                'topic_tags': get_topic_tags(page_source),\n",
    "                'accepted': get_accepted(page_source),\n",
    "                'submission': get_submission(page_source),\n",
    "                'solution': get_solution(page_source),\n",
    "                'discussion_count': get_discussion_count(page_source),\n",
    "                'likes': get_likes(page_source),\n",
    "                'dislikes': get_dislikes(page_source),\n",
    "                'similar_questions': get_similar_questions(page_source)\n",
    "            })\n",
    "\n",
    "            # Create a DataFrame for the current link and append it to the list\n",
    "            df = pd.DataFrame(data, index=[i])\n",
    "            dfs.append(df)\n",
    "            i += 1\n",
    "    print(dfs)\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a3d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape(df1, start=1, end=3000, file_name='x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('x2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e89194",
   "metadata": {},
   "source": [
    "<center> <h2 style=\"color:blue\"> Result of Second Section: </h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0324d3e",
   "metadata": {},
   "source": [
    "<p>This is the sample Dataframe : </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f6e4f",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/kKCZWQy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6d2c7",
   "metadata": {},
   "source": [
    "<h2> Third Section :</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe155d9",
   "metadata": {},
   "source": [
    "<p> Combine Both Part of The DataFrame: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03de23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df1.merge(df2, left_on='title', right_on='title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0a22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('leetcode_scraped_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bfc41",
   "metadata": {},
   "source": [
    "<center> <h2 style=\"color:blue\"> Result of Third Section: </h2> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bba1f2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px\"><b> Information about a DataFrame:</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd3e22",
   "metadata": {},
   "source": [
    "<p>This is the sample Dataframe : </p>\n",
    "\n",
    "```python\n",
    "df[:3].T\n",
    "```\n",
    "\n",
    "<img src=\"https://i.imgur.com/DJgcuOP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ed78b",
   "metadata": {},
   "source": [
    "- We have data of 3000 LeetCode problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a5e60",
   "metadata": {},
   "source": [
    "**Future Work**\n",
    "\n",
    "- Finding ways to improve upon the code and documentation of this project\n",
    "- Parallelizing the code to make the scaping process simpler and faster \n",
    "- Automating this script to run every week to get the latest data\n",
    "- Getting information about the premium data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8c17c",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. Selenium Documentation: https://www.selenium.dev/selenium/docs/api/py/api.html\n",
    "2. Selenium-Python Documentation https://selenium-python.readthedocs.io/\n",
    "3. Beautiful Soup Documentation https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "4. Stackoverflow https://stackoverflow.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
